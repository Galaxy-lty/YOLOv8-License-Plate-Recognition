import streamlit as st
from ultralytics import YOLO
from paddleocr import PaddleOCR
import cv2
import numpy as np
from PIL import Image
import tempfile
import os

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="è½¦ç‰Œæ£€æµ‹ä¸è¯†åˆ«ç³»ç»Ÿ", layout="wide")

# --- ä¾§è¾¹æ  ---
st.sidebar.title("è®¾ç½®")
conf_threshold = st.sidebar.slider("æ£€æµ‹ç½®ä¿¡åº¦ (Confidence)", 0.1, 1.0, 0.25)
st.sidebar.info("æ¨¡å‹åŠ è½½è‡ª:best.pt")

# --- æ ‡é¢˜ ---
st.title("ğŸš— æ·±åº¦å­¦ä¹ å¤§ä½œä¸š - è½¦ç‰Œæ£€æµ‹ä¸è¯†åˆ«ç³»ç»Ÿ")
st.markdown("### åŸºäº YOLOv8 (ç›®æ ‡æ£€æµ‹) + PaddleOCR (æ–‡å­—è¯†åˆ«)")


# --- åŠ è½½æ¨¡å‹ (åŠ ç¼“å­˜è£…é¥°å™¨ï¼Œé˜²æ­¢æ¯æ¬¡åˆ·æ–°éƒ½é‡æ–°åŠ è½½) ---
@st.cache_resource
def load_models():
    # åŠ è½½ä½ ä»Kaggleè®­ç»ƒå¥½çš„YOLOæ¨¡å‹
    det_model = YOLO('best.pt')
    # åŠ è½½OCRæ¨¡å‹ (è‡ªåŠ¨ä¸‹è½½è½»é‡çº§æ¨¡å‹)
    ocr_model = PaddleOCR(use_angle_cls=True, lang="ch")
    return det_model, ocr_model


with st.spinner('æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...'):
    model, ocr = load_models()

# --- ä¸Šä¼ å›¾ç‰‡ ---
uploaded_file = st.file_uploader("è¯·ä¸Šä¼ ä¸€å¼ åŒ…å«è½¦ç‰Œçš„å›¾ç‰‡ (æ”¯æŒ JPG, PNG)", type=['jpg', 'png', 'jpeg'])

if uploaded_file is not None:
    # 1. è¯»å–å›¾ç‰‡
    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
    image = cv2.imdecode(file_bytes, 1)  # BGRæ ¼å¼ (OpenCVæ ‡å‡†)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # RGBæ ¼å¼ (ç”¨äºæ˜¾ç¤º)

    # åˆ†åˆ—æ˜¾ç¤º
    col1, col2 = st.columns(2)

    with col1:
        st.image(image_rgb, caption="åŸå§‹å›¾ç‰‡", use_column_width=True)

    # 2. ç‚¹å‡»æ£€æµ‹æŒ‰é’®
    if st.button('å¼€å§‹æ£€æµ‹ä¸è¯†åˆ«', type="primary"):
        with st.spinner('æ­£åœ¨è¿›è¡Œæ·±åº¦å­¦ä¹ æ¨ç†...'):
            # YOLOæ¨ç†
            results = model(image, conf=conf_threshold)

            # ç”¨äºåœ¨åŸå›¾ä¸Šç”»æ¡†
            img_with_box = image.copy()

            recognized_text = []

            for result in results:
                boxes = result.boxes.xyxy.cpu().numpy()

                if len(boxes) == 0:
                    st.warning("æœªæ£€æµ‹åˆ°è½¦ç‰Œï¼Œè¯·è°ƒæ•´ç½®ä¿¡åº¦æˆ–æ›´æ¢å›¾ç‰‡ã€‚")

                for box in boxes:
                    x1, y1, x2, y2 = map(int, box)

                    # è£å‰ªè½¦ç‰ŒåŒºåŸŸ
                    plate_crop = image[y1:y2, x1:x2]

                    # OCR è¯†åˆ«
                    # è¿™é‡Œçš„ cls=True è¡¨ç¤ºå¯ç”¨æ–¹å‘åˆ†ç±»ï¼Œé˜²æ­¢è½¦ç‰Œæ­ªäº†è¯»ä¸å‡†
                    ocr_res = ocr.ocr(plate_crop, cls=True)

                    # å¤„ç†OCRç»“æœ
                    txt = "æœªè¯†åˆ«"
                    score = 0.0
                    if ocr_res and ocr_res[0]:
                        txt = ocr_res[0][0][1][0]
                        score = ocr_res[0][0][1][1]

                    recognized_text.append(f"ğŸ“ å†…å®¹: **{txt}** (å¯ä¿¡åº¦: {score:.2f})")

                    # åœ¨å›¾ä¸Šç”»æ¡†å’Œæ–‡å­—
                    cv2.rectangle(img_with_box, (x1, y1), (x2, y2), (0, 255, 0), 3)
                    # ä¸ºäº†é˜²æ­¢æ–‡å­—ä¹±ç ï¼Œå›¾ç‰‡ä¸Šåªç”»æ¡†ï¼Œæ–‡å­—åœ¨å³ä¾§æ˜¾ç¤º

            # æ˜¾ç¤ºç»“æœ
            with col2:
                st.image(cv2.cvtColor(img_with_box, cv2.COLOR_BGR2RGB), caption="æ£€æµ‹ç»“æœ", use_column_width=True)

            # æ˜¾ç¤ºè¯†åˆ«åˆ°çš„æ–‡æœ¬åˆ—è¡¨
            if recognized_text:
                st.success("æ£€æµ‹å®Œæˆï¼")
                for info in recognized_text:

                    st.markdown(info)

